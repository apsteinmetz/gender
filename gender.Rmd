---
title: "Gender Diversity in R and Python Package Development"
output: html_notebook
---
# Introduction

Over the last few years I have really enjoyed becoming part of the R community.  One of the best things about the community is the welcoming, inclusive and supportive nature of it.  I can't speak for other communities in the computer or data science worlds but I am well aware of the "brogrammer" culture in some circles that can be off-putting at times. The rise of codes of conduct across the open source world is changing things for the better, I think.

A couple months ago the creator of Python was interviewed saying [he thinks open source programming languages have a gender diversity problem](https://qz.com/1624252/pythons-creator-thinks-it-has-a-diversity-problem/).  This got me to thinking about whether the inclusive environment I observe in the R community is reflected in female contributions to popular packages and how it compares to the Python world.  Most of these packages are maintained on github which includes all the contributors who use the github environment to contribute.  Let's take a stab at identifying the gender of these contributors by name.

We will take a multi-stage approach to getting an answer to this question.

1. Get the names of the top packages in R and Python.
2. Identify which those packages which are maintained on Github.
3. Get the contributors to those packages (not as easy as it sounds).
4. Get baby names by gender from the U.S. Social Security database.
5. Decide whether a name is likely to be female or male.
6. Map all package conrtributors to gender, where possible.

As usual I follow a couple conventions.  The Tidyverse dialect is used throughout.  All functions to fetch data from the Web are wrapped in a test to see if the data was already retrieved.  This ensures that this notebook won't break if things in the wild change. In that event, you must get the data files from this github repo for this to work.

```{r message=FALSE, warning=FALSE}
library(tidyverse)
library(purrr)
library(jsonlite)
library(rvest)
# library(installr)
library(data.table) #for downloading CRAN/RStudio logs
library(httr)
library(gh)
library(formattable)

```
# Identify the Top Packages in R and Python

Use the cranlogs api from RStudio to get top package downloads from their CRAN mirror.  This is potentially a slow function but the top package downloads are pretty stable so we choose five randomly selected dates.

```{r get_top_R_packages}
# ----------------------------------------------------------------
#select 5 random days from the last six months
# Read data from RStudio site
# custom version of a  function from the installr package. See my Github repo.
source(file="download_RStudio_CRAN_data.R") 

if (!file.exists("data/r_pkg_list.rdata")) {
  RStudio_CRAN_dir <- download_RStudio_CRAN_data(START = Sys.Date()-180,END = Sys.Date(),sample=5)
  # read .gz compressed files form local directory
  RStudio_CRAN_data <- read_RStudio_CRAN_data(RStudio_CRAN_dir)
  
  dim(RStudio_CRAN_data)
  
  # Find the most downloaded packages
  r_pkg_list <- most_downloaded_packages(RStudio_CRAN_data,n=100) %>% 
    as_tibble(.name_repair = make.names,c("downloads")) %>% 
    rename(package=X)
  
  save(r_pkg_list,file="data/r_pkg_list.rdata")
} else load("data/r_pkg_list.rdata")


```

With Python the work as already been done for us here: https://hugovk.github.io/top-pypi-packages/. How helpful!

```{r get_top_python_packages}
if (!file.exists("data/python_pkg_list.rdata")){
  
  py_pkgs_raw<-read_json("https://hugovk.github.io/top-pypi-packages/top-pypi-packages-365-days.json",
                         simplifyVector = TRUE)
  python_pkg_list <- py_pkgs_raw$rows[1:100,] %>% 
    as_tibble() %>% 
    rename(package=project,downloads=download_count)
  save(python_pkg_list,file="data/python_pkg_list.rdata")
} else load("data/python_pkg_list.rdata")

```
# Get Contributor Names

This is the messy stuff.  We build functions to get contributors to packages and then the real names of those contributors.

We start with a search for the relevant repo with just repo name and optionally the language.  Suppose we want to know the names of the R `dplyr` contributors. The workflow looks like this:

Call the API endpoint.

https://api.github.com/search/repositories?q=dplyr+language:r

This will return at the top of the list:
      "full_name": "tidyverse/dplyr"
      
One problem I encountered is that not all R packages are tagged as being in the R language.  In particular, `Rcpp` and `data.table` are considered C language repos by Github.  This is one reason why not all the top packages appear to have Github repos. I manually grab the contributors for the two packages mentioned above but, out of laziness, I didn't go looking for any other missing packages.  As we will see, most of the top 100 packages for both languages are found so we have a fairly representative sample...I assume.

Once we have the full package name we can create URLs to get the usernames of all the contributors.

Contributor url: https://api.github.com/repos/tidyverse/dplyr/contributors

This JSON object will not contain the "real" names but the links to user profiles.  We have to make yet another call to the API to extract the real names.  Note some people use pseudonyms so the real name won't be available.

Calling the endpoint for the username "https://api.github.com/users/romainfrancois",

will return, among other things:

   "name": "Romain François"
   
Finally, we get what we are after!
   
NOTE: You will need a Github API key for this work.  Please refer to the documentation for the `gh` package.

The utility functions are below:
```{r get_package_contributors}
my_gh <- function(end_point) {
    return(jsonlite::fromJSON(jsonlite::toJSON(gh::gh(end_point)),simplifyVector = T))
}

json_to_df <- function(json){
    return(jsonlite::fromJSON(jsonlite::toJSON(json),simplifyVector = T))
}

# --------------------------------------------------------------------
get_contributor_ids <- function(target_repo){
# loop through all pages of contributors  
  search_url <- paste0("/repos/",
                       target_repo,
                       "/contributors")
  contributors_json <- gh(search_url)
  
  # return null in case of no contributors
  if (nchar(contributors_json[1])==0) return(NULL)
  
  contrib_node <- contributors_json
  repeat {
    contrib_node <- try(gh_next(contrib_node),silent=TRUE)
    if (is(contrib_node) == "try-error") break
    contributors_json <- c(contributors_json,contrib_node)  
  }

  contributor_ids <- json_to_df(contributors_json) %>%
    bind_rows() %>%   
    select(login,url,avatar_url)
  return(contributor_ids)
}

# ---------------------------------------------------------------------------
  get_name <- function(contrib_url){
    user_data <- my_gh(contrib_url)
    # just return login name if real name is missing
    if (is_empty(user_data$name)) return(user_data$login) else return(user_data$name)
  }

# --------------------------------------------------------------------
get_contrib_info <- function(repo_name="dplyr",language=NULL){
  print(repo_name)
  # we don't know the github username associated with the package to construct a search
  # to get the most likely candidate
  search_url <- paste0("/search/repositories?q=",
                       repo_name)
  if (!is.null(language)){
    search_url <- paste0(search_url,"+language:", language)
  }
  # first api call.
  repos <- my_gh(search_url) %>% .$items
  # return NULL if no repos in github are found
  if (length(repos) == 0) return(NULL)
  
  # get full path for exact match on repo name
  # there might be more than one user with repo of the same name
  # Since they will be in order of github "score", take just the first one
  target_repo <- repos %>% 
    select(name,full_name) %>% 
    filter(name == repo_name) %>%
    pull(full_name) %>% 
    .[1] %>% 
    unlist()
  # return NULL if no repos in github are found
  if (is.null(target_repo)) return(NULL)
  
  #second api call
  # get user urls for all contributors
  contributor_ids <- get_contributor_ids(target_repo)
  
  # return null in case of no contributors
  if (is.null(contributor_ids)) return(NULL)
  if (is.null(language)) language <- "none"
  contrib_names<-map(contributor_ids$url,get_name) %>% unlist()
  print(paste(length(contrib_names)," contributors"))
  contrib_info <- tibble(language=language,
                         package=repo_name,
                         path=target_repo,
                         contributor=contrib_names) %>% 
    bind_cols(contributor_ids) %>% 
    select(-url) %>% unnest()
  return(contrib_info)
}
```

Now let's do the work of iterating through the package lists.  As mentioned above, I get two packages manually before looping through the remaining packages.  I chose to use a `for` loop, as opposed to `map` or `apply` so we can save the intermediate results. It is a fairly slow process and you may reach your API data limit before finishing.  You don't want to start from scratch halfway through!  If you have to do this in multiple sessions, manually edit the package lists to include just what is left to retrieve.
```{r}
load("data/r_pkg_list.rdata")
if (!file.exists("data/r_pkg_contributors.rdata")){
  r_pkg_contributors <- NULL
# Rcpp package is categorized as C++, not R, langauge so get it manually.
  contrib_info_rcpp <- get_contrib_info("Rcpp")
  contrib_info_rcpp <- contrib_info_rcpp %>% mutate(language = "r")
  r_pkg_contributors <- bind_rows(r_pkg_contributors,contrib_info_rcpp)
  r_pkg_list <- r_pkg_list %>% filter(package != "Rcpp")

  # use for loop instead of map or apply so we can save intermediate steps
  for(pkg in r_pkg_list$package) {
    r_pkg_contributors <- r_pkg_contributors %>% 
      bind_rows(get_contrib_info(pkg,language="r"))
    save(r_pkg_contributors,file="data/r_pkg_contributors.rdata")
  }
} else load("data/r_pkg_contributors.rdata")

load("data/python_pkg_list.rdata")
if (!file.exists("data/python_pkg_contributors.rdata")){
  python_pkg_contributors <- NULL
  for(pkg in python_pkg_list$package) {
    python_pkg_contributors <- python_pkg_contributors %>% 
      bind_rows(get_contrib_info(pkg,language="python"))
    save(python_pkg_contributors,file="data/python_pkg_contributors.rdata")
  }  
} else load("data/python_pkg_contributors.rdata")
```
# Analysis

Now that we have the package contributor database. Do some analysis.

```{r}
load("data/r_pkg_contributors.rdata")

#summarize what we found
r_pkg_contributors %>% 
  group_by(package) %>% 
  summarise(contributors=n()) %>% 
  arrange(desc(contributors)) %>% 
  summary()
```

There are 73 out of the top 100 R packages with repos we *easily* found on Github (remember, I'm lazy).  The median number of contributors is 10. 300 people have contributed to the `fs` package, which is implements the linux file library `libuv`.

```{r}

```

How did we do with the Python packages?
```{r}
load("data/python_pkg_contributors.rdata")

#summarize what we found
python_pkg_contributors %>% 
  group_by(package) %>% 
  summarise(contributors=n()) %>% 
  arrange(desc(contributors)) %>% 
  summary()
```

Right off the bat, it looks like Python package development is more of a community effort.  The median package has 44 contributors.  The venerable `matplotlib` takes the prize of most contributors at 429.

Let's merge the two datasets to simply handling.
```{r}
pkg_contributors <- bind_rows(r_pkg_contributors,python_pkg_contributors)
```

Let's look at the most contributed-to packages. Remember this is NOT a ranking of the most popular packages. It is a ranking of the number of contributors among the most popular packages.  For R, the takeaway is that the Tidyverse is very much a shared effort.
```{r}
pkg_contributors %>% 
  filter(language=="r") %>% 
  group_by(package) %>%
  summarise(contributors=n()) %>% 
  arrange(contributors) %>%
  top_n(10,wt=contributors) %>% 
  ggplot(aes(as_factor(package),contributors)) + 
  geom_col()+ 
  labs(main = "R Packages", x = "Package Name",y = "Contributor Count") +
  coord_flip()
```
```{r}
pkg_contributors %>% 
  filter(language=="python") %>% 
  group_by(package) %>%
  summarise(contributors=n()) %>% 
  arrange(contributors) %>%
  top_n(10,wt=contributors) %>% 
  ggplot(aes(as_factor(package),contributors)) + 
  geom_col()+ 
  labs(main = "Python Packages", x = "Package Name",y = "Contributor Count") +
  coord_flip()
```

Who are the most prolific contributors among the top packages?  We note, with R, that many of the top packages are part of the tidyverse ecosystem and will have a very high degree of overlap among package contributors.  No one tidyverse package acts as a proxy for the rest, however.

## Most Prolific R Package Contributors
```{r}
pkg_contributors %>% 
  filter(language=="r") %>% 
  group_by(contributor) %>% 
  summarise(packages=n()) %>% 
  arrange(desc(packages))
```
## Most Prolific Python Package Contributors
```{r}
pkg_contributors %>% 
  filter(language=="python") %>% 
  group_by(contributor) %>% 
  summarise(packages=n()) %>% 
  arrange(desc(packages))
```

Looking at the number of contributors for the top packages in both languages, we find that Python packages tend to have many more contributors.
```{r}
summary_contrib<-pkg_contributors %>% 
  group_by(language,package) %>% 
  summarise(contributors=n()) %>% 
  arrange(desc(contributors)) %>% 
  rownames_to_column(var="index")

summary_contrib %>% 
  ungroup() %>% 
  filter(language=="r") %>% 
  select(contributors) %>%
  rownames_to_column(var="index") %>% 
  arrange(contributors) %>% 
  
#  ggplot(aes(index,contributors,group=language,color=language)) + 
  ggplot(aes(index,contributors)) + 
  geom_col(size=2) + 
  theme(axis.text.x=element_blank()) +
  labs(main="Top Python Packages Show More Collaboration",
       x="Packages in Order of Contributors",
       y="Number of Contributors")
```


Who has contributed to both top R and Python packages?  Grouping by login name ensures that we don't get two different people with the same name.  There are 44 people who have contributed to some of both the top Python and R packages.
```{r}
c_p <- python_pkg_contributors %>% 
  group_by(login,contributor) %>% 
  summarise(python_packages=n())

c_r <- r_pkg_contributors %>% 
  group_by(login,contributor) %>% 
  summarise(r_packages=n())

inner_join(c_r,c_p,by=c("contributor","login")) %>% 
  arrange(desc(r_packages))
```

# Try to Determine Gender of Contributors

We use the Social Security baby names database for 1990.  It is important to be aware of the limitations of this.

1. I used 1990 because I guess that is close to the average birth year of most package contributors.  Is it?

2. The dataset contains registered births for only the United States.  Many contributors were born, or live today, outside the U.S. The U.S, while more of a melting pot than many countries, will have a subset of global names.

3. Transliteration of names from languages that don't use Western characters don't follow hard and fast rules. The same name might be transliterated multiple ways. "Sergey" or "Sergei?"

4. Ordering of surname and given name.  Chinese names typically are reported surname first.  Many Chinese people follow western conventions in global settings but maybe not.  I may be tagging the surname as the given name in some (many?) cases.

5. Many names are used for "both" (yes, I know) genders.  I choose an aribitrary ratio of gender predominance of 75% to pronounce certainty.  Noteworthy: "Hadley" is in our "Uncertain" bucket.

6. Gender identity becomes a choice at some age.  People may choose (or not choose) a gender  inconsistant with the identification in this dataset.

7. Some people use pseudonyms that are not common names.

Knowing all that, let's plunge on.

```{r}
load("data/names_90.rdata")
names_90 <- names_90 %>% 
  rename(first = Name) %>%
  mutate(first = tolower(first)) %>% 
  select(first,Gender,Count) %>% 
  spread(Gender,Count) %>% 
  mutate_if(is.numeric, ~replace(., is.na(.), 0)) %>% 
  mutate(prob_female=F/(F+M))

cutoff = 0.75 # threshhold probability for calling gender
names_90 <- names_90 %>% mutate(gender="Uncertain")
names_90 <- names_90 %>% mutate(gender=if_else(prob_female>cutoff,"Female",gender))
names_90 <- names_90 %>% mutate(gender=if_else(prob_female<(1-cutoff),"Male",gender))
names_90_subset <- names_90 %>% select(first,gender)
```

Now let's join the baby names to our contributors.
```{r}
r_pkg_contributors <-r_pkg_contributors %>%
  separate("contributor",into=c("first"),remove=FALSE,extra="drop")

python_pkg_contributors <-python_pkg_contributors %>%
  separate("contributor",into=c("first"),remove=FALSE,extra="drop")

r_genders <- r_pkg_contributors %>% 
  select(-path,-avatar_url,-login) %>% 
  mutate(first = tolower(first)) %>% 
  left_join(names_90_subset) %>% 
  mutate_all(~replace(., is.na(.),"Uncertain")) 

python_genders <- python_pkg_contributors %>% 
  select(-path,-avatar_url,-login) %>% 
  mutate(first = tolower(first)) %>% 
  left_join(names_90_subset) %>% 
  mutate_all(~replace(., is.na(.),"Uncertain")) 

```

```{r}
agg_gender <- bind_rows(python_genders,r_genders) %>% 
  select(language,gender) %>% 
  table() 
agg_gender %>% plot(main="Gender Representation in Package Contributions")
```
For our ultimate conclusion, let's assume that the "Uncertain" gender breaks into male and female in the same proportions that already exist.
```{r}
agg_gender <- bind_rows(python_genders,r_genders) %>% 
  filter(gender != "Uncertain") %>% 
  select(language,gender) %>% 
  table() %>% prop.table(margin=1) 

percent(agg_gender,digits = 0)
```
There it is.  This was certainly a lot of work to get to a four cell crosstab but we have our answer. Women contribute to the top R packages at more than twice the rate of top Python packages.  Can we speculate as to a reason?  R is almost exclusively a data science language and most of the top packages reflect that. Python is more of a general purpose language that is also quite popular for data science, but as we look down the list of most popular python packages we see more utility packages.  Perhaps women are less represented in general computer science than they are in data science. With both languages, more than 90% of the contributors are men.  Clearly, we have a way to go with gender diversity in both communities.  Narrowing down the package list to focus on just data science packages is an avenue for further exploration.

There are other dimensions of diversity we might look at that are beyond the ability to infer from names.  It would be nice if we could see actual images of all contributors so we might make some observations about racial diversity or remove some of the amiguities around gender identification.  This approach would come with its own set of challenges and risks, however.

As mentioned at the start of this ariticle there are many reasons to take our conclusions with a grain of salt but the broad results conform with what we might intuitively expect.
